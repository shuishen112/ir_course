{"cells":[{"cell_type":"markdown","metadata":{"id":"j1OPmSmUh73u"},"source":["# NIR 2022 - Lab 7: Learning to Rank in PyTerrier\n","\n","Learning to Rank (LTR) refers to the application of re-ranking a candidate set of retrieved documents.\n","\n","By manually engineering features and assigning them to each document, LTR techniques aim at getting the top-ranked documents ranked correctly.\n","Three main types of loss functions are used:\n","- Pointwise: One instance of the set is considered at a time, predicting how relevant it is in the current query. At inference, use predicted relevance scores for each document to order the set.\n","- Pairwise: A pair of instances is chosen and the order of those two is predicted. At inference, repeat this for each pair of documents for the given query to find the final order of the entire query.\n","- Listwise: Find the optimal order (most relevant document at the top of the ranking) by considering many or all instances at once.\n","\n","Different models are commonly used: linear, trees and neural networks.\n","\n","In this lab, we will look at tree-based approaches, trained using either pointwise or listwise learning objectives.\n","\n","The material for this lab is largely based on the PyTerrier ECIR 2021 tutorial."]},{"cell_type":"markdown","metadata":{"id":"7eWl0NQJpUev"},"source":["## Data and PyTerrier Setup"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ohSXudmfAK4u"},"outputs":[],"source":["# Mount Google Drive\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":375,"status":"ok","timestamp":1623144359949,"user":{"displayName":"Emanuele Bugliarello","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSNW1w1ACVsfSmcOAWsNVsKiUAe5DPIDstrtlOuA=s64","userId":"07893326812593803856"},"user_tz":-120},"id":"jSiydrggA6bD","outputId":"eb9f2abe-1766-48b5-d2ba-6d1a9c2c789f"},"outputs":[],"source":["# Check that you can `ls` your directory with NIR material\n","# !ls \"/content/drive/My Drive/nir2021\""]},{"cell_type":"code","execution_count":13,"metadata":{"id":"1SFOIBi0BCGL"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2453, 2)\n","     docno                                               text\n","0   935016  he emigrated to france with his family in 1956...\n","1  2360440  after being ambushed by the germans in novembe...\n","2   347765  she was the second ship named for captain alex...\n","3  1969335  world war ii was a global war that was under w...\n","4  1576938  the ship was ordered on 2 april 1942 laid down...\n","(9, 2)\n","       qid                 query\n","0  1015979    president of chile\n","1     2674    computer animation\n","2   340095  2020 summer olympics\n","3  1502917         train station\n","4     2574       chinese cuisine\n","(2454, 4)\n","       qid    docno label iteration\n","0  1015979  1015979     2         0\n","1  1015979  2226456     1         0\n","2  1015979  1514612     1         0\n","3  1015979  1119171     1         0\n","4  1015979  1053174     1         0\n"]}],"source":["# Load the data\n","import pandas as pd\n","\n","# BASEDIR = \"/content/drive/My Drive/nir2021/\"\n","BASEDIR = './'\n","# corpus\n","docs_df = pd.read_csv(BASEDIR + 'data/lab_docs.csv', dtype=str)\n","print(docs_df.shape)\n","print(docs_df.head())\n","\n","# topics\n","topics_df = pd.read_csv(BASEDIR + 'data/lab_topics.csv', dtype=str)\n","print(topics_df.shape)\n","print(topics_df.head())\n","\n","# Load qrels\n","qrels_df = pd.read_csv(BASEDIR + 'data/lab_qrels.csv',dtype=str)\n","print(qrels_df.shape)\n","print(qrels_df.head())"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"f93sZgvb-AaF"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Ignoring invalid distribution -orch (/maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n","\u001b[0mCollecting python-terrier==0.5.0\n","  Downloading python-terrier-0.5.0.tar.gz (74 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.1/74.1 KB\u001b[0m \u001b[31m839.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (1.22.1)\n","Requirement already satisfied: pandas in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (1.4.0)\n","Requirement already satisfied: wget in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (3.2)\n","Requirement already satisfied: pytrec_eval>=0.5 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (0.5)\n","Requirement already satisfied: tqdm in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (4.63.0)\n","Requirement already satisfied: pyjnius~=1.3.0 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (1.3.0)\n","Requirement already satisfied: matchpy in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (0.5.5)\n","Requirement already satisfied: sklearn in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (0.0)\n","Requirement already satisfied: deprecation in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (2.1.0)\n","Requirement already satisfied: chest in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (0.2.3)\n","Requirement already satisfied: scipy in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (1.7.3)\n","Requirement already satisfied: requests in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (2.27.1)\n","Requirement already satisfied: nptyping in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (1.4.4)\n","Requirement already satisfied: more_itertools in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (8.12.0)\n","Requirement already satisfied: ir_datasets>=0.2.0 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (0.5.1)\n","Requirement already satisfied: jinja2 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (3.0.3)\n","Requirement already satisfied: statsmodels in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from python-terrier==0.5.0) (0.13.1)\n","Requirement already satisfied: zlib-state>=0.1.3 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from ir_datasets>=0.2.0->python-terrier==0.5.0) (0.1.5)\n","Requirement already satisfied: warc3-wet>=0.2.3 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from ir_datasets>=0.2.0->python-terrier==0.5.0) (0.2.3)\n","Requirement already satisfied: ijson>=3.1.3 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from ir_datasets>=0.2.0->python-terrier==0.5.0) (3.1.4)\n","Requirement already satisfied: lz4>=3.1.1 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from ir_datasets>=0.2.0->python-terrier==0.5.0) (3.1.10)\n","Requirement already satisfied: lxml>=4.5.2 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from ir_datasets>=0.2.0->python-terrier==0.5.0) (4.7.1)\n","Requirement already satisfied: trec-car-tools>=2.5.4 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from ir_datasets>=0.2.0->python-terrier==0.5.0) (2.6)\n","Requirement already satisfied: pyautocorpus>=0.1.1 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from ir_datasets>=0.2.0->python-terrier==0.5.0) (0.1.8)\n","Requirement already satisfied: unlzw3>=0.2.1 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from ir_datasets>=0.2.0->python-terrier==0.5.0) (0.2.1)\n","Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from ir_datasets>=0.2.0->python-terrier==0.5.0) (0.2.5)\n","Requirement already satisfied: beautifulsoup4>=4.4.1 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from ir_datasets>=0.2.0->python-terrier==0.5.0) (4.10.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from ir_datasets>=0.2.0->python-terrier==0.5.0) (6.0)\n","Requirement already satisfied: six>=1.7.0 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from pyjnius~=1.3.0->python-terrier==0.5.0) (1.16.0)\n","Requirement already satisfied: cython in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from pyjnius~=1.3.0->python-terrier==0.5.0) (0.29.27)\n","Requirement already satisfied: certifi>=2017.4.17 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from requests->python-terrier==0.5.0) (2021.10.8)\n","Requirement already satisfied: idna<4,>=2.5 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from requests->python-terrier==0.5.0) (3.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from requests->python-terrier==0.5.0) (1.26.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from requests->python-terrier==0.5.0) (2.0.11)\n","Requirement already satisfied: heapdict in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from chest->python-terrier==0.5.0) (1.0.1)\n","Requirement already satisfied: packaging in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from deprecation->python-terrier==0.5.0) (21.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from jinja2->python-terrier==0.5.0) (2.0.1)\n","Requirement already satisfied: multiset<3.0,>=2.0 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from matchpy->python-terrier==0.5.0) (2.1.1)\n","Requirement already satisfied: typish>=1.7.0 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from nptyping->python-terrier==0.5.0) (1.9.3)\n","Requirement already satisfied: pytz>=2020.1 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from pandas->python-terrier==0.5.0) (2021.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from pandas->python-terrier==0.5.0) (2.8.2)\n","Requirement already satisfied: scikit-learn in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from sklearn->python-terrier==0.5.0) (1.0.2)\n","Requirement already satisfied: patsy>=0.5.2 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from statsmodels->python-terrier==0.5.0) (0.5.2)\n","Requirement already satisfied: soupsieve>1.2 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from beautifulsoup4>=4.4.1->ir_datasets>=0.2.0->python-terrier==0.5.0) (2.3.1)\n","Requirement already satisfied: cbor>=1.0.0 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from trec-car-tools>=2.5.4->ir_datasets>=0.2.0->python-terrier==0.5.0) (1.0.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from packaging->deprecation->python-terrier==0.5.0) (3.0.7)\n","Requirement already satisfied: joblib>=0.11 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from scikit-learn->sklearn->python-terrier==0.5.0) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages (from scikit-learn->sklearn->python-terrier==0.5.0) (3.1.0)\n","Building wheels for collected packages: python-terrier\n","  Building wheel for python-terrier (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for python-terrier: filename=python_terrier-0.5.0-py3-none-any.whl size=79551 sha256=e20f25114a9044a5dfc23d44b39ba92f227409e82064b2b3f2b8c06d13de11cd\n","  Stored in directory: /maps/projects/futhark1/data/wzm289/.cache/pip/wheels/9f/f3/5f/4c8a196749598775e042028034c1c87b2e1525543481446b15\n","Successfully built python-terrier\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: python-terrier\n","  Attempting uninstall: python-terrier\n","\u001b[33m    WARNING: Ignoring invalid distribution -orch (/maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m    Found existing installation: python-terrier 0.8.1\n","    Uninstalling python-terrier-0.8.1:\n","      Successfully uninstalled python-terrier-0.8.1\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n","\u001b[0mSuccessfully installed python-terrier-0.5.0\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/maps/projects/futhark1/data/wzm289/miniconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# !pip install python-terrier==0.5.0"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"kJBb2x_ICWWd"},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTerrier 0.8.1 has loaded Terrier 5.6 (built by craigmacdonald on 2021-09-17 13:27)\n","\n","No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"]}],"source":["# Init PyTerrier\n","import pyterrier as pt\n","if not pt.started():\n","    pt.init()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"MsN_8hg6Ca8e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of documents: 2453\n","Number of terms: 23693\n","Number of postings: 208487\n","Number of fields: 0\n","Number of tokens: 273373\n","Field names: []\n","Positions:   true\n","\n"]}],"source":["# Build index\n","indexer = pt.DFIndexer(BASEDIR + \"./indexes/default\", overwrite=True, blocks=True)\n","index_ref = indexer.index(docs_df[\"text\"], docs_df[\"docno\"])\n","index = pt.IndexFactory.of(index_ref)\n","print(index.getCollectionStatistics().toString())"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"8_n60xKOCefY"},"outputs":[],"source":["# Build IR systems\n","tf = pt.BatchRetrieve(index, wmodel=\"Tf\")\n","tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n","bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")"]},{"cell_type":"markdown","metadata":{"id":"084Iw6Tc2sow"},"source":["## Learning to Rank\n","\n","We will now look at how to construct, train and evaluate LTR pipelines in PyTerrier."]},{"cell_type":"markdown","metadata":{"id":"qckDD2p43iYA"},"source":["### Data Splitting\n","\n","First, let's split out topics into train, validation and test sets. \n","\n","Our lab data only has 9 topics, which is ridiculously small for training. \n","We will split these into: 4 for training, 2 for validation and 3 for evaluation."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"TjbZcTm--K9z"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","SEED=42\n","\n","tr_val_topics, test_topics = train_test_split(topics_df, test_size=3, random_state=SEED)\n","train_topics, valid_topics = train_test_split(tr_val_topics, test_size=2, random_state=SEED)"]},{"cell_type":"markdown","metadata":{"id":"mF4xedde4r9i"},"source":["### Feature Set\n","\n","In order to learn a mapping between a document and its relevance score for a given query, our LTR model needs query-document features.\n","That is, each query-document pair is represented by a multi-dimensional feature vector (each dimension of the vector is a feature) indicating how relevant or important the document is with respect to the query with respect to each feature.\n","\n","Here, for the sake of simplicity, we only consider three features:\n","1. the BM25 score;\n","2. the TF score;\n","3. the TF-IDF score.\n","\n","In your project, you should explore more established information retrieval features (see the [LETOR paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/letor3.pdf)) and more relevant features (e.g. was the article published after 2019?).\n","\n","Today, we will re-rank the top-K (STAGE1_CUTOFF) documents for each query and evaluate the top-100 (STAGE2_CUTOFF) ones."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"7xCPMpZf64l6"},"outputs":[],"source":["STAGE1_CUTOFF = 100\n","\n","# We retrieve the top (% operator) STAGE1_CUTOFF documents\n","# And we concatenate (** operator) their BM25, TF and TF-IDF scores as features\n","ltr_feats1 = (bm25 % STAGE1_CUTOFF) >> (bm25 ** tf ** tfidf)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Wl6J-Hle8iF4"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>qid</th>\n","      <th>docid</th>\n","      <th>docno</th>\n","      <th>rank</th>\n","      <th>score</th>\n","      <th>query</th>\n","      <th>features</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>236</td>\n","      <td>234372</td>\n","      <td>0</td>\n","      <td>4.455773</td>\n","      <td>train</td>\n","      <td>[4.455772798261049, 12.0, 2.983525429580879]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2057</td>\n","      <td>1418389</td>\n","      <td>1</td>\n","      <td>4.185537</td>\n","      <td>train</td>\n","      <td>[4.185537262862662, 6.0, 2.8025793561742307]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1801</td>\n","      <td>2400360</td>\n","      <td>2</td>\n","      <td>4.177455</td>\n","      <td>train</td>\n","      <td>[4.177454903290807, 7.0, 2.7971675171049095]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>2095</td>\n","      <td>1441398</td>\n","      <td>3</td>\n","      <td>4.149063</td>\n","      <td>train</td>\n","      <td>[4.149062711261565, 7.0, 2.778156487872484]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>1005</td>\n","      <td>2373010</td>\n","      <td>4</td>\n","      <td>3.942148</td>\n","      <td>train</td>\n","      <td>[3.942147700083634, 5.0, 2.639608984316581]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>1</td>\n","      <td>2231</td>\n","      <td>1129049</td>\n","      <td>95</td>\n","      <td>3.031105</td>\n","      <td>train</td>\n","      <td>[3.0311049015705924, 2.0, 2.029587001629092]</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>1</td>\n","      <td>18</td>\n","      <td>1556726</td>\n","      <td>96</td>\n","      <td>3.023598</td>\n","      <td>train</td>\n","      <td>[3.0235984921969976, 2.0, 2.0245608110522957]</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>1</td>\n","      <td>728</td>\n","      <td>1556725</td>\n","      <td>97</td>\n","      <td>3.023598</td>\n","      <td>train</td>\n","      <td>[3.0235984921969976, 2.0, 2.0245608110522957]</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>1</td>\n","      <td>151</td>\n","      <td>2029140</td>\n","      <td>98</td>\n","      <td>3.016129</td>\n","      <td>train</td>\n","      <td>[3.0161291696212325, 2.0, 2.0195594532956265]</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>1</td>\n","      <td>880</td>\n","      <td>2144173</td>\n","      <td>99</td>\n","      <td>3.016129</td>\n","      <td>train</td>\n","      <td>[3.0161291696212325, 2.0, 2.0195594532956265]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 7 columns</p>\n","</div>"],"text/plain":["   qid  docid    docno  rank     score  query  \\\n","0    1    236   234372     0  4.455773  train   \n","1    1   2057  1418389     1  4.185537  train   \n","2    1   1801  2400360     2  4.177455  train   \n","3    1   2095  1441398     3  4.149063  train   \n","4    1   1005  2373010     4  3.942148  train   \n","..  ..    ...      ...   ...       ...    ...   \n","95   1   2231  1129049    95  3.031105  train   \n","96   1     18  1556726    96  3.023598  train   \n","97   1    728  1556725    97  3.023598  train   \n","98   1    151  2029140    98  3.016129  train   \n","99   1    880  2144173    99  3.016129  train   \n","\n","                                         features  \n","0    [4.455772798261049, 12.0, 2.983525429580879]  \n","1    [4.185537262862662, 6.0, 2.8025793561742307]  \n","2    [4.177454903290807, 7.0, 2.7971675171049095]  \n","3     [4.149062711261565, 7.0, 2.778156487872484]  \n","4     [3.942147700083634, 5.0, 2.639608984316581]  \n","..                                            ...  \n","95   [3.0311049015705924, 2.0, 2.029587001629092]  \n","96  [3.0235984921969976, 2.0, 2.0245608110522957]  \n","97  [3.0235984921969976, 2.0, 2.0245608110522957]  \n","98  [3.0161291696212325, 2.0, 2.0195594532956265]  \n","99  [3.0161291696212325, 2.0, 2.0195594532956265]  \n","\n","[100 rows x 7 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Example of stage1 output\n","ltr_feats1.search(\"train\")"]},{"cell_type":"markdown","metadata":{"id":"aXLmdQiV9j2J"},"source":["### Learning\n","\n","We now train two learning to rank techniques:\n","- [Random forests from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), a pointwise regression tree technique.\n","- LambdaMART from [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html), a pairwise regression tree technique.\n","\n","In each case, we take our feature pipeline, `ltr_feats1`, and we compose it (`>>` operator) with the learned model. \n","We use PyTerrier's `pt.ltr.apply_learned_model()` interface to directly access the different learners.\n","\n","The full pipeline is then fitted (learned) using `.fit()`, specifying the training topics and qrels. \n","\n","Importantly, the preceeding stages of the pipeline (retrieval and feature calculation) are applied to the training topics in order to obtained the results, which are then passed to the learning to rank technique.\n","\n","**NB:** Usually, only the documents with associated train qrels are used for LTR. This means that a small K (STAGE1_CUTOFF) might lead to fewer observed query-document-score data points. On the other hand, choosing a large K might be unfeasible due to long training time.\n"]},{"cell_type":"markdown","metadata":{"id":"t55keLy_y2ac"},"source":["#### Bootstrap Aggregation: Random Forest\n","\n","Random forest is a supervised learning algorithm that relies on ensemble learning method for classification and regression.\n","\n","Decision trees in random forests are run in parallel, with no interaction between any two trees while building them.\n","After constructing a multitude of decision trees at training time, a random forest outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n","\n","To prevent growing highly correlated trees, random forests introduce two modifications:\n","- The number of features that can be split on at each node can be limited to some percentage of the total (a hyperparameter), ensuring a fair use of all potentially predictive features.\n","- Each tree can draw a random sample from the original dataset when generating its splits (known as bootstrapping), adding a further element of randomness that prevents overfitting."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"UVg5p8OJ-7Rj"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.1s finished\n"]}],"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","rf = RandomForestRegressor(n_estimators=100, verbose=1, random_state=SEED, n_jobs=2)\n","\n","rf_pipe = ltr_feats1 >> pt.ltr.apply_learned_model(rf)\n","\n","rf_pipe.fit(train_topics, qrels_df)"]},{"cell_type":"markdown","metadata":{"id":"flBXcxwe1XnM"},"source":["#### Boosting: LambdaMART\n","\n","Boosting refers to algorithms that utilize weighted averages to make weak learners into stronger learners. In boosting, each model that runs, defines which features the next model will focus on.\n","That is, a model is learnt from another, which in turn boosts the learning.\n","\n","In this lab, we will use [LightGBM](https://github.com/microsoft/LightGBM) to implement LambdaMART, a pairwise technique based on gradient boosted decision trees with a cost function derived from LambdaRank.\n","\n","Light GBM (LGBM) is a gradient boosting framework that uses tree based learning algorithm.\n","LGBM can handle large data and it is memory efficient. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning for quick development.\n","However, it is not advisable to use LGBM on small datasets as it is sensitive to overfitting.\n","While the implementation of LGBM is easy, hyperparameter tuning may not. Check out [this blogpost](https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc) for a good description of available parameters.\n","\n","\n","Another popular library for gradient boosting algorithms is [XGBoost](https://github.com/dmlc/xgboost).\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"0pKrXZSM_Jd4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/wzm289/miniconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n","  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n","/home/wzm289/miniconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:621: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n","  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n","/home/wzm289/miniconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:621: UserWarning: Found 'ndcg_at' in params. Will use it instead of 'eval_at' argument\n","  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n","/home/wzm289/miniconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:621: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n","  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n","/home/wzm289/miniconda3/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n","  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] early_stopping_round is set=5, early_stopping_rounds=5 will be ignored. Current value: early_stopping_round=5\n","[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005388 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 223\n","[LightGBM] [Info] Number of data points in the train set: 349, number of used features: 3\n","[LightGBM] [Warning] early_stopping_round is set=5, early_stopping_rounds=5 will be ignored. Current value: early_stopping_round=5\n","[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[1]\tvalid_0's ndcg@10: 0.689492\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[2]\tvalid_0's ndcg@10: 0.668603\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[3]\tvalid_0's ndcg@10: 0.668603\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[4]\tvalid_0's ndcg@10: 0.668603\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[5]\tvalid_0's ndcg@10: 0.668603\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[6]\tvalid_0's ndcg@10: 0.668603\n"]}],"source":["import lightgbm as lgb\n","\n","# this configures LightGBM as LambdaMART\n","lmart_l = lgb.LGBMRanker(\n","    task=\"train\",\n","    silent=False,\n","    min_data_in_leaf=1,\n","    min_sum_hessian_in_leaf=1,\n","    max_bin=255,\n","    num_leaves=31,\n","    objective=\"lambdarank\",\n","    metric=\"ndcg\",\n","    ndcg_eval_at=[10],\n","    ndcg_at=[10],\n","    eval_at=[10],\n","    learning_rate=0.1,\n","    importance_type=\"gain\",\n","    num_iterations=10,\n","    early_stopping_rounds=5\n",")\n","\n","lmart_x_pipe = ltr_feats1 >> pt.ltr.apply_learned_model(lmart_l, form=\"ltr\", fit_kwargs={'eval_at':[10]})\n","\n","# LightGBM has early stopping enabled, which uses a validation topics set\n","lmart_x_pipe.fit(train_topics, qrels_df, valid_topics, qrels_df)"]},{"cell_type":"markdown","metadata":{"id":"_6aa1826AOX7"},"source":["### Evaluation\n","\n","Finally, we now compare our ranking pipelines on our 3 test topics with the BM25 baseline. In all cases, we rank only 100 (STAGE2_CUTOFF) results per query.\n","\n","We'll report MAP, NDCG and NDCG@10 measures as well as mean response time (`\"mrt\"`)."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"bSOmtp56_Jbe"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>map</th>\n","      <th>ndcg</th>\n","      <th>ndcg_cut_10</th>\n","      <th>mrt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>BM25</td>\n","      <td>0.319667</td>\n","      <td>0.528478</td>\n","      <td>0.818269</td>\n","      <td>17.131405</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>BM25 + RF</td>\n","      <td>0.262618</td>\n","      <td>0.496678</td>\n","      <td>0.761424</td>\n","      <td>88.714593</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>BM25 + Lmart</td>\n","      <td>0.223307</td>\n","      <td>0.478142</td>\n","      <td>0.671745</td>\n","      <td>54.551590</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           name       map      ndcg  ndcg_cut_10        mrt\n","0          BM25  0.319667  0.528478     0.818269  17.131405\n","1     BM25 + RF  0.262618  0.496678     0.761424  88.714593\n","2  BM25 + Lmart  0.223307  0.478142     0.671745  54.551590"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["STAGE2_CUTOFF = 100\n","qrels_df = qrels_df.astype({'label': 'int32'})\n","pt.Experiment(\n","    [bm25 % STAGE2_CUTOFF, rf_pipe % STAGE2_CUTOFF, lmart_x_pipe % STAGE2_CUTOFF],\n","    test_topics,\n","    qrels_df, \n","    names=[\"BM25\", \"BM25 + RF\", \"BM25 + Lmart\"],\n","    eval_metrics=[\"map\", \"ndcg\", \"ndcg_cut_10\", \"mrt\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMzwp5Gx_JZP"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"NIR2021-Lab7.ipynb","provenance":[],"toc_visible":true},"interpreter":{"hash":"d8ed0073c41b023cdd22b6268c6b3f0c9b6d97a6234cf243afa672a36c79f6c6"},"kernelspec":{"display_name":"Python 3.9.5 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}
